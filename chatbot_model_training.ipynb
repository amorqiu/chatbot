{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize #this package tokenize word by whitespace and punctuation\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_read = pd.read_csv('chatbot.tsv', engine='python',sep='\\n|\\t',encoding=\"utf8\") #read large tsv file\n",
    "# tsv_read.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distinct messages and responses from tsv file \n",
    "message=tsv_read[['message_id','message']].drop_duplicates()\n",
    "responses=tsv_read[['response_id','response']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17373, 3)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating=pd.read_csv('aggregated-hw3-ratings.train.csv',encoding=\"utf8\",header=None) # read from training dataset\n",
    "rating.shape #(17373, 3)\n",
    "# rating.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating=rating.rename(columns={0: \"message_id\", 1: \"response_id\", 2:'rating'})\n",
    "rating_df=rating.merge(message,how='left',on='message_id')\n",
    "rating_df=rating_df.merge(responses,how='left',on='response_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get corpus for training set and full set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=responses[['response','response_id']].drop_duplicates('response').reset_index()\n",
    "# documents\n",
    "# len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses=documents['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Processing functions to get bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_url(file):\n",
    "    file_lowered=file.lower() \n",
    "    file_url=re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', file_lowered, flags=re.MULTILINE) #remove url\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  \n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', file_url)\n",
    "# lower vocab and remove url and emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pn(tokens):\n",
    "    #remove punctuations & numbers\n",
    "    filtered_words = [word for word in tokens if word.isalpha()]\n",
    "    return filtered_words\n",
    "# remove punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(tokens):\n",
    "    f = open(\"stoplist.txt\", \"r\")\n",
    "    stoplist=f.read()\n",
    "    words = [w for w in tokens if not w in stoplist]\n",
    "    return words\n",
    "# remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "def get_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "def lemma_token(tokens):\n",
    "    tagged_words=nltk.pos_tag(tokens)\n",
    "    new_token = []\n",
    "    for i in tagged_words:\n",
    "        wordnet_pos = get_pos(i[1]) #or wordnet.NOUN\n",
    "        new_token.append(wnl.lemmatize(i[0],pos=wordnet_pos))\n",
    "    return new_token\n",
    "# lemmetize my bag of word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get index for train documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_doc = [\n",
    "    [word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in test_documents\n",
    "]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency_test = defaultdict(int)\n",
    "for text in texts_doc:\n",
    "    for token in text:\n",
    "        frequency_test[token] += 1\n",
    "\n",
    "texts_doc = [\n",
    "    [token for token in text if frequency_test[token] > 1]\n",
    "    for text in texts_doc\n",
    "]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts_doc)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Index for all documents\n",
    "this is when I use whole documents to get ranked responses. It turned out not really good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = defaultdict(int)\n",
    "for text in cleandb:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in cleandb\n",
    "]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus_all = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "lsi = models.LsiModel(corpus_all, id2word=dictionary, num_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_id=[]\n",
    "for i in testing['messagd_id'].unique():\n",
    "        query=message[message['message_id']==i].message.values[0]\n",
    "        vec_bow = dictionary.doc2bow(query.lower().split())\n",
    "        vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n",
    "        sims = index[vec_lsi] \n",
    "        newsim = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "        for doc in newsim[0:10]:\n",
    "            #append response_id and \n",
    "            response_id.append((i,documents.iloc[doc[0]].response_id))\n",
    "        print(\"finished get documents for query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(response_id, columns=[\"message_id\", \"response_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('kaggle_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing=pd.read_csv('aggregated-hw3-rating.test.csv',encoding=\"utf8\")\n",
    "message_list=testing['messagd_id'].unique() #get all messages in test file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=message_list[0]\n",
    "message_db=tsv_read[tsv_read['message_id']==i]\n",
    "test_response=message_db['response']\n",
    "lowered_doc=[lower_url(i) for i in test_response]\n",
    "tokened_doc=[word_tokenize(i) for i in lowered_doc]\n",
    "removed=[remove_pn(i) for i in tokened_doc]\n",
    "cleandb=[remove_stop(i) for i in removed]\n",
    "pure_db=[lemma_token(i) for i in cleandb]      \n",
    "query=message_db['message'].unique()[0]\n",
    "tokenized_query = query.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_db\n",
    "bm25=BM25Okapi(pure_db)\n",
    "# len(test_response)\n",
    "result=bm25.get_top_n(tokenized_query, test_response.to_list(), n=10)\n",
    "for doc in result:\n",
    "#         print(doc)\n",
    "    id=message_db[message_db['response']==doc].response_id.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_25result=[]\n",
    "for i in message_list:\n",
    "    message_db=tsv_read[tsv_read['message_id']==i]\n",
    "    test_response=message_db['response']\n",
    "    lowered_doc=[lower_url(i) for i in test_response]\n",
    "    tokened_doc=[word_tokenize(i) for i in lowered_doc]\n",
    "    removed=[remove_pn(i) for i in tokened_doc]\n",
    "    cleandb=[remove_stop(i) for i in removed]\n",
    "    pure_db=[lemma_token(i) for i in cleandb]      \n",
    "    query=message_db['message'].unique()[0]\n",
    "    tokenized_query = query.split(\" \")\n",
    "    if len(pure_db)<10:\n",
    "        print(\"not enough\")\n",
    "        message_db=tsv_read[['response','response_id']].sample(n=50).drop_duplicates('response_id')\n",
    "        test_response=message_db['response']\n",
    "        lowered_doc=[lower_url(i) for i in test_response]\n",
    "        tokened_doc=[word_tokenize(i) for i in lowered_doc]\n",
    "        removed=[remove_pn(i) for i in tokened_doc]\n",
    "        cleandb=[remove_stop(i) for i in removed]\n",
    "        pure_db=[lemma_token(i) for i in cleandb] \n",
    "        bm25 = BM25Okapi(pure_db)\n",
    "        \n",
    "    else:\n",
    "        bm25=BM25Okapi(pure_db)\n",
    "    result=bm25.get_top_n(tokenized_query, test_response.to_list(), n=10)\n",
    "    for doc in result:\n",
    "#         print(doc)\n",
    "        id=message_db[message_db['response']==doc].response_id.values[0]\n",
    "        bm_25result.append((i,id))\n",
    "    print('finishing getting query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_25result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_25 = pd.DataFrame(bm_25result, columns=[\"message_id\", \"response_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_25.to_csv('kaggle_1116.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Gensim-similarity query to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(test_response,query):\n",
    "#     texts_doc = [\n",
    "#       [word for word in document.lower().split() if word not in stoplist]\n",
    "#       for document in test_response]\n",
    "\n",
    "# remove words that appear only once\n",
    "    frequency_test = defaultdict(int)\n",
    "    for text in test_response:\n",
    "        for token in text:\n",
    "            frequency_test[token] += 1\n",
    "\n",
    "    texts_doc = [\n",
    "        [token for token in text if frequency_test[token] > 1]\n",
    "        for text in test_response\n",
    "        ]\n",
    "\n",
    "    dictionary = corpora.Dictionary(texts_doc)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts_doc]\n",
    "    lsi_new = models.LsiModel(corpus, id2word=dictionary, num_topics=3)\n",
    "    index = similarities.MatrixSimilarity(lsi_new[corpus])  \n",
    "    vec_bow = dictionary.doc2bow(query.lower().split())\n",
    "    vec_lsi = lsi_new[vec_bow]  # convert the query to LSI space\n",
    "    sims_1 = index[vec_lsi] \n",
    "    newsim = sorted(enumerate(sims_1), key=lambda item: -item[1])\n",
    "   \n",
    "    return newsim # get  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_db=tsv_read[tsv_read['message_id']==message_list[268]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>message</th>\n",
       "      <th>response_id</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dtncx79</td>\n",
       "      <td>Is the second test goint to be option 2? I thi...</td>\n",
       "      <td>dtndoho</td>\n",
       "      <td>Where is this second test? I’m confused</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  message_id                                            message response_id  \\\n",
       "0    dtncx79  Is the second test goint to be option 2? I thi...     dtndoho   \n",
       "\n",
       "                                  response  \n",
       "0  Where is this second test? I’m confused  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result=[]\n",
    "\n",
    "for i in message_list:\n",
    "    message_db=tsv_read[tsv_read['message_id']==i].reset_index(drop=True)\n",
    "    test_response=message_db['response']\n",
    "    lowered_doc=[lower_url(i) for i in test_response]\n",
    "    tokened_doc=[word_tokenize(i) for i in lowered_doc]\n",
    "    removed=[remove_pn(i) for i in tokened_doc]\n",
    "    cleandb=[remove_stop(i) for i in removed]\n",
    "    pure_db=[lemma_token(i) for i in cleandb]    \n",
    "    query=message_db['message'].unique()[0]\n",
    "    if len(test_response)<10:\n",
    "        message_db=tsv_read[['response','response_id']].drop_duplicates('response_id')\n",
    "        test_response=message_db['response']\n",
    "#         for i in range(0,10):\n",
    "#             new_result.append((i,'none'))\n",
    "#         print(\"not enough query\")\n",
    "# #         message_db=tsv_read[['response','response_id']].drop_duplicates('response_id')\n",
    "# #         test_response=message_db['response']\n",
    "    sim=get_corpus(pure_db,query)\n",
    "    for doc in sim[0:10]:\n",
    "        new_result.append((i,message_db.iloc[doc[0]].response_id))\n",
    "    print('finish getting query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(new_result, columns=[\"message_id\", \"response_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get prediction of result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('kaggle_1114.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
